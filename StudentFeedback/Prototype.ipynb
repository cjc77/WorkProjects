{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses = pd.read_csv(\"Spreadsheets/A.tsv\", sep='\\t')\n",
    "\n",
    "# Import training data - CANNOT match your target data\n",
    "responses_train = pd.read_csv(\"Spreadsheets/B.tsv\", sep='\\t')\n",
    "\n",
    "# Define global constant, class size\n",
    "CLASS_SIZE = 45\n",
    "\n",
    "# Define global constants: WORDS and BIGRAMS\n",
    "WORDS = 20\n",
    "BIGRAMS = 1000\n",
    "\n",
    "# Define global constants for parts of speech\n",
    "NOUN = 'N'\n",
    "ADJ = 'J'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Find Response Counts for Each of the Sections</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question1</th>\n",
       "      <th>Question2</th>\n",
       "      <th>Question3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Question1  Question2  Question3\n",
       "Section                                             \n",
       "1                22         22         16         12\n",
       "2                31         30         27         18\n",
       "3                32         31         27         25\n",
       "4                28         28         24         18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_rates = responses.groupby('Section').count()\n",
    "# response_rates.loc['Total'] = response_rates.sum()\n",
    "# response_rates['Max'] = response_rates.max(axis=1)\n",
    "response_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Response Rate by Column</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 responses: 111\n",
      "Q2 responses: 94\n",
      "Q3 responses: 73\n",
      "Q1 response rate: 0.616700\n",
      "Q2 response rate: 0.522200\n",
      "Q3 response rate: 0.405600\n",
      "Response Rate (responded to >= 1 qestion): 0.616700\n"
     ]
    }
   ],
   "source": [
    "# Find out how many classes by counting rows in the 'Section' column\n",
    "num_sections = response_rates.shape[0]\n",
    "total_students = CLASS_SIZE * num_sections\n",
    "\n",
    "# All responses [column] -> [string list]\n",
    "# not pd.isnull(...) -> ignore NaN fields\n",
    "# List of response strings by columns: Q1..Q3\n",
    "q1_feedback = responses['Question1'].tolist()\n",
    "q1_feedback = [w for w in q1_feedback if not pd.isnull(w)]\n",
    "q2_feedback = responses['Question2'].tolist()\n",
    "q2_feedback = [w for w in q2_feedback if not pd.isnull(w)]\n",
    "q3_feedback = responses['Question3'].tolist()\n",
    "q3_feedback = [w for w in q3_feedback if not pd.isnull(w)]\n",
    "\n",
    "q1_responses = len(q1_feedback)\n",
    "q2_responses = len(q2_feedback)\n",
    "q3_responses = len(q3_feedback)\n",
    "\n",
    "# Which question had the most responses?\n",
    "max_responses = max(q1_responses, q2_responses, q3_responses)\n",
    "\n",
    "# Find response rates for all questions\n",
    "max_resp_rate = round((max_responses / total_students),4)\n",
    "q1_response_rate = round((q1_responses / total_students), 4)\n",
    "q2_response_rate = round((q2_responses / total_students), 4)\n",
    "q3_response_rate = round((q3_responses / total_students), 4)\n",
    "\n",
    "print('Q1 responses: %d'% q1_responses, 'Q2 responses: %d' % q2_responses,\n",
    "     'Q3 responses: %d' % q3_responses, 'Q1 response rate: %f' % q1_response_rate,\n",
    "      'Q2 response rate: %f' % q2_response_rate,\n",
    "      'Q3 response rate: %f' % q3_response_rate,\n",
    "      'Response Rate (responded to >= 1 qestion): %f'\n",
    "      % max_resp_rate, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Positive/Negative Response Word Counts & Word Lengh</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Q1 word count: 1787\n",
      "Avg. Q1 comment word count: 16.099099\n",
      "Avg. Q1 word length: 6.574706\n",
      "\n",
      "Total Q2 word count: 2097\n",
      "Avg. Q2 comment word count: 22.308511\n",
      "Avg. Q2 word length: 6.567477\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find average character counts for responses\n",
    "q1_char_cnts = [len(w) for w in q1_feedback]\n",
    "q2_char_cnts = [len(w) for w in q2_feedback]\n",
    "q1_char_avg = np.mean(q1_char_cnts)\n",
    "q2_char_avg = np.mean(q2_char_cnts)\n",
    "\n",
    "# Find average word counts for responses\n",
    "q1_wdcnts = [len(tokenizer.tokenize(w)) for w in q1_feedback]\n",
    "q2_wdcnts = [len(tokenizer.tokenize(w)) for w in q2_feedback]\n",
    "q1_wdcnt_avg = np.mean(q1_wdcnts)\n",
    "q2_wdcnt_avg = np.mean(q2_wdcnts)\n",
    "\n",
    "\n",
    "# Show results\n",
    "print(\"Total Q1 word count: %d\" % sum(q1_wdcnts))\n",
    "print(\"Avg. Q1 comment word count: %f\" % q1_wdcnt_avg)\n",
    "print(\"Avg. Q1 word length: %f\" % (q1_char_avg / q1_wdcnt_avg))\n",
    "print()\n",
    "print(\"Total Q2 word count: %d\" % sum(q2_wdcnts))\n",
    "print(\"Avg. Q2 comment word count: %f\" % q2_wdcnt_avg)\n",
    "print(\"Avg. Q2 word length: %f\" % (q2_char_avg / q2_wdcnt_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create Training Data</h1>\n",
    "<p>***This data should NOT be the same as the file in question</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_train = responses_train['Question1'].tolist()\n",
    "q1_train = [w for w in q1_train if not pd.isnull(w)]\n",
    "\n",
    "q2_train = responses_train['Question2'].tolist()\n",
    "q2_train = [w for w in q2_train if not pd.isnull(w)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create Strings from Feedback Columns</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert response lists to 2 long strings\n",
    "q1_string = \" \".join(q1_feedback)\n",
    "q2_string = \" \".join(q2_feedback)\n",
    "\n",
    "# Training data, too\n",
    "q1_train_string = \" \".join(q1_train)\n",
    "q2_train_string = \" \".join(q2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part of Speech Tagging</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# Train on training string\n",
    "# Create tokenizer for q1 and q2\n",
    "q1_sent_tokenizer = PunktSentenceTokenizer(q1_train_string)\n",
    "q2_sent_tokenizer = PunktSentenceTokenizer(q2_train_string)\n",
    "\n",
    "\n",
    "#### Q1\n",
    "\n",
    "# Tokenize and then create part of speech tags - [(word, POS), ...]\n",
    "q1_tokenized = q1_sent_tokenizer.tokenize(q1_string)\n",
    "q1_tagged = []\n",
    "\n",
    "# Append tagged tuples to q1_tagged\n",
    "for w in q1_tokenized:\n",
    "    words = nltk.word_tokenize(w)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    q1_tagged.append(tagged)\n",
    "\n",
    "# Collapse the 2-D list into 1-D\n",
    "q1_tagged = [tup for sent in q1_tagged for tup in sent]\n",
    "   \n",
    "\n",
    "    \n",
    "#### Q2\n",
    "\n",
    "# Tokenize and then create part of speech tags - [(word, POS), ...]\n",
    "q2_tokenized = q2_sent_tokenizer.tokenize(q2_string)\n",
    "q2_tagged = []\n",
    "\n",
    "# Append tagged tuples to q2_tagged\n",
    "for w in q2_tokenized:\n",
    "    words = nltk.word_tokenize(w)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    q2_tagged.append(tagged)\n",
    "    \n",
    "# Collapse 2-D list into 1-D\n",
    "q2_tagged = [tup for sent in q2_tagged for tup in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find 10 Most Frequent Nouns and Adjectives for Q1 and Q2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q1:  \n",
      "\n",
      "Nouns:  [('Sed', 41), ('lacinia', 25), ('sem', 25), ('vel', 25), ('lectus', 25), ('Nulla', 24), ('quis', 24), ('tortor', 22), ('quam', 22), ('diam', 22), ('ante', 22), ('mattis', 21), ('cursus', 21), ('Proin', 19), ('non', 19), ('ipsum', 19), ('Morbi', 18), ('ligula', 18), ('Vestibulum', 18), ('suscipit', 18)] \n",
      "\n",
      "Adjectives:  [('Aenean', 16), ('ut', 15), ('iaculis', 15), ('nunc', 9), ('scelerisque', 8), ('aliquet', 8), ('urna', 8), ('ipsum', 7), ('tellus', 6), ('pede', 5), ('pharetra', 5), ('interdum', 5), ('augue', 4), ('accumsan', 4), ('nec', 3), ('sem', 3), ('ullamcorper', 3), ('nibh', 3), ('luctus', 2), ('amet', 2)]\n",
      "\n",
      "\n",
      "Q2:  \n",
      "\n",
      "Nouns:  [('Sed', 53), ('Nulla', 33), ('ante', 32), ('et', 31), ('quis', 30), ('quam', 26), ('Morbi', 25), ('ultrices', 24), ('non', 23), ('tortor', 22), ('ipsum', 22), ('Integer', 22), ('augue', 22), ('Vestibulum', 22), ('Ut', 22), ('diam', 22), ('lacinia', 21), ('cursus', 21), ('vel', 20), ('Curabitur', 20)] \n",
      "\n",
      "Adjectives:  [('ut', 16), ('iaculis', 13), ('nec', 11), ('aliquet', 10), ('Aenean', 10), ('ipsum', 8), ('urna', 7), ('tellus', 6), ('nunc', 6), ('sollicitudin', 6), ('egestas', 6), ('nibh', 5), ('vel', 5), ('amet', 5), ('est', 4), ('scelerisque', 4), ('accumsan', 4), ('augue', 4), ('faucibus', 4), ('pharetra', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "\n",
    "# Filter target parts of speech\n",
    "q1_nouns = [item[0] for item in q1_tagged if item[1][0] == NOUN]\n",
    "q1_adjs = [item[0] for item in q1_tagged if item[1][0] == ADJ]\n",
    "\n",
    "\n",
    "# Find the 10 most frequently occuring words from each part of speech\n",
    "q1_noun_fdist = FreqDist(q1_nouns)\n",
    "q1_adj_fdist = FreqDist(q1_adjs)\n",
    "\n",
    "# Find 20 most common nouns & adjectives and their frequencies\n",
    "q1_common_nouns = q1_noun_fdist.most_common(WORDS)\n",
    "q1_common_adjs = q1_adj_fdist.most_common(WORDS)\n",
    "\n",
    "print('\\n\\nQ1: ', '\\n\\nNouns: ', q1_common_nouns, '\\n\\nAdjectives: ',\n",
    "      q1_common_adjs)\n",
    "\n",
    "\n",
    "# Q2\n",
    "\n",
    "# Filter target parts of speech\n",
    "q2_nouns = [item[0] for item in q2_tagged if item[1][0] == NOUN]\n",
    "q2_adjs = [item[0] for item in q2_tagged if item[1][0] == ADJ]\n",
    "\n",
    "# Find the 10 most frequently occuring words from each part of speech\n",
    "q2_noun_fdist = FreqDist(q2_nouns)\n",
    "q2_adj_fdist = FreqDist(q2_adjs)\n",
    "\n",
    "# Find 20 most common nouns & adjectives and their frequencies\n",
    "q2_common_nouns = q2_noun_fdist.most_common(WORDS)\n",
    "q2_common_adjs = q2_adj_fdist.most_common(WORDS)\n",
    "\n",
    "print('\\n\\nQ2: ', '\\n\\nNouns: ', q2_common_nouns, '\\n\\nAdjectives: ',\n",
    "      q2_common_adjs)\n",
    "\n",
    "\n",
    "# Make sets out of the most common words\n",
    "q1_word_set = set()\n",
    "for i in range(WORDS):\n",
    "    q1_word_set.add(q1_common_nouns[i][0])\n",
    "    q1_word_set.add(q1_common_adjs[i][0])\n",
    "\n",
    "q2_word_set = set()\n",
    "for i in range(WORDS):\n",
    "    q2_word_set.add(q2_common_nouns[i][0])\n",
    "    q2_word_set.add(q2_common_adjs[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Finding Keywords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class aptent taciti', 'metus vitae pharetra auctor', 'integer lacinia sollicitudin massa', 'per ince gestas porttitor', 'sed dignissim lacinia nunc', 'vestibulum ante ipsum primis', 'sodales libero eget ante', 'cursus ipsum ante rci', 'vestibulum lacinia accumsan porttitor', 'curabitur sit amet mauris', 'quisque volutpat condimentum velit', 'purus al les ligula', 'tortor neque adipiscing diam', 'sed aliq nostra', 'urna non tincidunt mattis', 'su nam nec ante', 'ut ultrices ultrices enim', 'praesent mauri aliquet eget', 'donec lacus nun ue', 'lorem ipsum dolor imperdiet', 'sed pretium blandit orci', 'maec ia molestie dui', 'augue congue elementum', 'donec acinia nunc', 'maecenas aliquet mollis lectus', 'ultrices sit amet', 'aliq nvallis tristique sem', 'sed convallis tristique sem', 'curabitur tor enean quam', 'lorem dapibus diam', 'tellus consequat imperdiet', 'per inceptos himenaeos', 'sem massa mattis sem', 'donec lacus nunc', 'lacinia molestie dui', 'nunc feugiat mi', 'pede suscipit sodales', 'curabitur sodales ligula', 'ut orci risus', 'ut eu diam', 'ue adipiscing diam', 'nibh elemen augue', 'us luctus magna', 'nulla ut felis', 'nam nec ante', 'sed aliquet risus', 'nulla quis sem', 'tel et nulla', 'nulla metus metus', 'integer e corper', 'purus aliquam imperdiet', 'fusce ac e', 'na luctus suscipit', 'nibh elementum imperdiet', 'vestibulum nisi lectus', 'curabitur tor disse', 'sed non quam', 'aenean lectus elit', 'lacinia nunc', 'consectetur adipiscing elit', 'integer id quam', 'se cipit sodales', 'morbi lect magna', 'morbi lectus risus', 'duis sagittis ipsum', 'sed lacinia', 'onvallis tristique sem', 'convallis tristique sem', 'etiam quisque cursus', 'ssa mattis sem', 'tincidunt sed', 'nulla fac potenti', 'nullam mauris orci', 'metus vitae', 'na vestibulum sapien', 'aliquet eget', 'ultrices sit', 'quisque nisl felis', 'egestas e', 'dales lig uismod', 'facilisis laoreet', 'sus sequat imperdiet', 'ut fringilla', 'sed lectus', 'praesent blandit dolor', 'ullamcorper vel', 'proin quam', 'cursus quis', 'vestibulum ante', 'aliquet et', 'blandit vel', 'facilisis ac', 'luctus non', 'viverra vitae', 'nulla facilisi', 'mauris ipsum', 'accumsan porttitor', 'suscipit quis', 'tortor neque', 'nam nec', 'metus tortor', 'metus metus', 'quisque cursus', 'morbi mi', 'iaculis vel', 'aenean laoreet', 'sed nisi', 'nulla quam', 'nunc feugi', 'curabitur tortor', 'iaculis et', 'viverra nec', 'vestibulum sapien', 'maecenas mattis', 'sed convalli', 'scing diam', 'us risus', 'aenean quam', 'etiam ultrices', 'mauris massa', 'suspendisse potenti', 'cras metus', 'venenatis tristique', 'dunt mattis', 'convallis id', 'quat imperdiet', 'fermentum non', 'scelerisque sem', 'ultricies eu', 'suscipit assa', 'pellentesque nibh', 'praesent mauris', 'curabitur lementum', 'commodo ac', 'sus diet', 'viv ero', 'susc em']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "# Q1\n",
    "q1_rake = Rake()\n",
    "\n",
    "q1_rake.extract_keywords_from_text(q1_string)\n",
    "q1_keywords = q1_rake.get_ranked_phrases()\n",
    "new_keywords = []\n",
    "\n",
    "r = re.compile(r'[{}]'.format(punctuation))\n",
    "for s in q1_keywords:\n",
    "    new_strs = r.sub(' ', s)\n",
    "    if 2 <= len(new_strs.split()) <= 4:\n",
    "        new_keywords.append(s)\n",
    "\n",
    "print(new_keywords)\n",
    "        \n",
    "# Q2\n",
    "q2_rake = Rake()\n",
    "\n",
    "q2_rake.extract_keywords_from_text(q2_string)\n",
    "q2_keywords = q2_rake.get_ranked_phrases()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Finding Bigrams and Collocations</h1>\n",
    "\n",
    "<p>Trigrams did not end up being very informative</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( eu pede )\n",
      "( pulvinar ullamcorper )\n",
      "( lacinia sollicitudin )\n",
      "( risus accumsan )\n",
      "( sit amet )\n",
      "( auctor sem )\n",
      "( interdum magna )\n",
      "( pharetra auctor )\n",
      "( augue semper )\n",
      "( nec tellus )\n",
      "( tellus consequat )\n",
      "( sed augue )\n",
      "( tellus sed )\n",
      "( ante ipsum )\n",
      "( ipsum primis )\n",
      "( viverra nec )\n",
      "( pede facilisis )\n",
      "( Nam nec )\n",
      "( amet pede )\n",
      "( aliquet mollis )\n",
      "( nunc egestas )\n",
      "( sit amet )\n",
      "( In scelerisque )\n",
      "( Pellentesque nibh )\n",
      "( Fusce nec )\n",
      "( nibh elementum )\n",
      "( amet augue )\n",
      "( augue congue )\n",
      "( magna augue )\n",
      "( sodales ligula )\n",
      "( sem massa )\n",
      "( vitae pharetra )\n",
      "( accumsan porttitor )\n",
      "( ipsum sit )\n",
      "( pede Ut )\n",
      "( Nulla metus )\n",
      "( metus ullamcorper )\n",
      "( Integer lacinia )\n",
      "( Vestibulum ante )\n",
      "( pede suscipit )\n",
      "( Vestibulum sapien )\n",
      "( ipsum Nulla )\n",
      "( sapien Proin )\n",
      "( urna non )\n",
      "( fermentum non )\n",
      "( orci aliquet )\n",
      "( ut felis )\n",
      "( nunc Curabitur )\n",
      "( eget ante )\n",
      "( quam Etiam )\n",
      "( ullamcorper Nulla )\n",
      "( eu diam )\n",
      "( dapibus diam )\n",
      "( tortor Lorem )\n",
      "( tortor Pellentesque )\n",
      "( aliquet Mauris )\n",
      "( ipsum Praesent )\n",
      "( nibh Aenean )\n",
      "( quis est )\n",
      "( aliquet et )\n",
      "( Nulla facilisi )\n",
      "( quam In )\n",
      "( lacinia urna )\n",
      "( lectus Vivamus )\n",
      "( mollis lectus )\n",
      "( scelerisque sem )\n",
      "( vel sus )\n",
      "( a tellus )\n",
      "( ullamcorper vel )\n",
      "( vel nunc )\n",
      "( lacinia arcu )\n",
      "( nec ante )\n",
      "( at pede )\n",
      "( at interdum )\n",
      "( at nibh )\n",
      "( risus iaculis )\n",
      "( adipiscing diam )\n",
      "( Maecenas mattis )\n",
      "( Morbi mi )\n",
      "( ut ligula )\n",
      "( porttitor Morbi )\n",
      "( Lorem ipsum )\n",
      "( Proin ut )\n",
      "( lectus elit )\n",
      "( quis turpis )\n",
      "( turpis Nulla )\n",
      "( tristique sem )\n",
      "( ligula in )\n",
      "( elementum Morbi )\n",
      "( sodales Aenean )\n",
      "( tincidunt mattis )\n",
      "( nisi Nulla )\n",
      "( augue eget )\n",
      "( non tincidunt )\n",
      "( magna luctus )\n",
      "( dignissim lacinia )\n",
      "( lacus luctus )\n",
      "( lacus nunc )\n",
      "( nibh Quisque )\n",
      "( tortor neque )\n",
      "( lacinia nunc )\n",
      "( iaculis vel )\n",
      "( Quisque cursus )\n",
      "( imperdiet Vestibulum )\n",
      "( vitae ligula )\n",
      "( cursus ipsum )\n",
      "( amet mauris )\n",
      "( aliquet eget )\n",
      "( Maecenas aliquet )\n",
      "( ligula vel )\n",
      "( Sed nisi )\n",
      "( Sed pretium )\n",
      "( libero Sed )\n",
      "( non convallis )\n",
      "( ipsum ante )\n",
      "( nunc viverra )\n",
      "( suscipit sodales )\n",
      "( sem at )\n",
      "( in ipsum )\n",
      "( lectus risus )\n",
      "( et iaculis )\n",
      "( iaculis et )\n",
      "( mattis tortor )\n",
      "( lectus Integer )\n",
      "( Mauris ipsum )\n",
      "( sagittis ipsum )\n",
      "( sem Proin )\n",
      "( dui quis )\n",
      "( luctus suscipit )\n",
      "( aliquet risus )\n",
      "( Aenean quam )\n",
      "( luctus magna )\n",
      "( Proin quam )\n",
      "( a cursus )\n",
      "( diam a )\n",
      "( Aenean laoreet )\n",
      "( cursus metus )\n",
      "( Morbi lectus )\n",
      "( Aenean lectus )\n",
      "( ante quis )\n",
      "( non massa )\n",
      "( quis aliquet )\n",
      "( Nulla ut )\n",
      "( eget diam )\n",
      "( Sed dignissim )\n",
      "( Curabitur tortor )\n",
      "( suscipit quis )\n",
      "( porttitor cursus )\n",
      "( vel suscipit )\n",
      "( luctus non )\n",
      "( quis luctus )\n",
      "( massa Vestibulum )\n",
      "( ligula Nulla )\n",
      "( Morbi in )\n",
      "( id quam )\n",
      "( Sed convallis )\n",
      "( mauris Morbi )\n",
      "( massa mattis )\n",
      "( justo Sed )\n",
      "( in nibh )\n",
      "( ipsum dolor )\n",
      "( et tortor )\n",
      "( Vestibulum lacinia )\n",
      "( lacinia aliquet )\n",
      "( mattis Sed )\n",
      "( Sed aliquet )\n",
      "( suscipit Sed )\n",
      "( Sed lacinia )\n",
      "( ante Sed )\n",
      "( mattis sem )\n",
      "( turpis quis )\n",
      "( diam Vestibulum )\n",
      "( diam at )\n",
      "( tortor Integer )\n",
      "( Sed lectus )\n",
      "( Nulla quam )\n",
      "( ante Nulla )\n",
      "( metus Sed )\n",
      "( cursus quis )\n",
      "( vel tincidunt )\n",
      "( quam Aenean )\n",
      "( a tortor )\n",
      "( quam Morbi )\n",
      "( quis ligula )\n",
      "( ligula lacinia )\n",
      "( Nulla quis )\n",
      "( quis sem )\n",
      "( diam Sed )\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "# get rid of punctuation\n",
    "punct = {',', '.', ';', '-', '!'}\n",
    "q1_tagged_nopunct = [tup for tup in q1_tagged if tup[0] not in punct]\n",
    "q2_tagged_nopunct = [tup for tup in q2_tagged if tup[0] not in punct]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "bi_finder = BigramCollocationFinder.from_words(q1_tagged_nopunct)\n",
    "tri_finder = TrigramCollocationFinder.from_words(q1_tagged_nopunct)\n",
    "\n",
    "# # Find trigrams/bigrams\n",
    "# tri_finder.nbest(trigram_measures.pmi, 40)\n",
    "# bi_finder.nbest(bigram_measures.pmi, 40)\n",
    "\n",
    "# Find most frequent bigrams/trigrams\n",
    "bi_finder.apply_freq_filter(2)\n",
    "# bi_finder.nbest(bigram_measures.pmi, 100)\n",
    "\n",
    "tri_finder.apply_freq_filter(2)\n",
    "# tri_finder.nbest(trigram_measures.pmi, 100)\n",
    "\n",
    "\n",
    "# Filter out stopwords - 1000, should be longer than the max # of bigrams\n",
    "bi_finder.apply_word_filter(lambda w: w in stop_words)\n",
    "bigrams = bi_finder.nbest(bigram_measures.pmi, BIGRAMS)\n",
    "\n",
    "for i in range(len(bigrams)):\n",
    "    if bigrams[i][0][0] in q1_word_set or bigrams [i][1][0] in q1_word_set:\n",
    "        print('(', bigrams[i][0][0], bigrams[i][1][0], ')')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
